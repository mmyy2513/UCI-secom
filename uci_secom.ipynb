{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "uci-secom.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqB-7R2DmyJg"
      },
      "source": [
        "!export TF_CPP_MIN_LOG_LEVEL=2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC0ObhkRtsq8"
      },
      "source": [
        "# Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6iccjaSx4Ns"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from copy import deepcopy\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "import tempfile\n",
        "from pandas.core.common import SettingWithCopyWarning\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "import warnings\n",
        "warnings.simplefilter(action = 'ignore', category = SettingWithCopyWarning)\n",
        "warnings.simplefilter(action = 'ignore', category = FutureWarning)\n",
        "warnings.simplefilter(action = 'ignore', category = DataConversionWarning)\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.decomposition import PCA, FastICA\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
        "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm as SVM\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1zv-i0gtx_T"
      },
      "source": [
        "# Define Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gotUsPVt-jB"
      },
      "source": [
        "### Feature / Label 분리 및 병합"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIZIHYPfuJSJ"
      },
      "source": [
        "def featureLabelSplit(dataframe, label = 'Pass/Fail'):\r\n",
        "  X = dataframe.drop([label],axis = 1)\r\n",
        "  y = dataframe[label]\r\n",
        "  return (X,y)\r\n",
        "\r\n",
        "def featureLabelMerge(feature_df, label_df, label_index = 'Pass/Fail'):\r\n",
        "  feature_df[label_index] = label_df\r\n",
        "  return feature_df\r\n",
        "\r\n",
        "def passFailSplit(dataframe):\r\n",
        "  pass_data = dataframe[dataframe['Pass/Fail']==-1]\r\n",
        "  fail_data = dataframe[dataframe['Pass/Fail']== 1]\r\n",
        "  return (pass_data,fail_data)\r\n",
        "  \r\n",
        "def passFailMerge(pass_df,fail_df):\r\n",
        "  return pd.concat([pass_df,fail_df])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTdxI-oguZDd"
      },
      "source": [
        "### 일정 상관관계 이상의 Feature 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML17PkFpuYgG"
      },
      "source": [
        "def get_high_corr_indicies(dataframe, correaltion_score = 0.05):\r\n",
        "  corr = dataframe.corr(method = 'pearson')\r\n",
        "  corr_label = abs(corr[\"Pass/Fail\"])\r\n",
        "  high_corr_features = corr_label[corr_label > correaltion_score]\r\n",
        "  return high_corr_features"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6ZXNfGOue2F"
      },
      "source": [
        "### 한가지 값의 Feature 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9h9MgoquYB5"
      },
      "source": [
        "def removeUnique(dataframe, except_index_list = []):\r\n",
        "  df_fill = dataframe.fillna(method = 'ffill')\r\n",
        "  df_fill = df_fill.fillna(method = 'bfill')\r\n",
        "  single_unique_columns = []\r\n",
        "  for i in dataframe.columns:\r\n",
        "    if ((len(df_fill[i].unique()) == 1)  and  ((except_index_list.index.isin([i])).any() == False)):\r\n",
        "      single_unique_columns.append(i)\r\n",
        "  removed_df = dataframe.drop(single_unique_columns,1)\r\n",
        "  return removed_df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqSRAIMHu_6_"
      },
      "source": [
        "### 일정 비율 이상 Null값을 가지는 Column 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7tTB6Z6vGzS"
      },
      "source": [
        "def dropNullDominant(dataframe,except_index_list = [],ratio = 0.5):\r\n",
        "  null_dominant_columns = []\r\n",
        "  for i in dataframe.columns:\r\n",
        "    if dataframe[i].isna().sum() > ratio * (dataframe.shape[0])and ((except_index_list.index.isin([i])).any() == False):\r\n",
        "      null_dominant_columns.append(i)\r\n",
        "  drop_null_dominant_df = dataframe.drop(null_dominant_columns, axis = 1)\r\n",
        "  return drop_null_dominant_df"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HroQ3rLKvIb8"
      },
      "source": [
        "### Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8ssI6eJvNPZ"
      },
      "source": [
        "def robustScaler(dataframe):\r\n",
        "  RS = RobustScaler()\r\n",
        "  rs_data = RS.fit_transform(dataframe)\r\n",
        "  rs_df = pd.DataFrame(data = rs_data, columns = dataframe.columns)\r\n",
        "  return rs_df"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzm_u7WOvQLf"
      },
      "source": [
        "### PowerTransform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po8d1xGIvQha"
      },
      "source": [
        "def powerTransform(dataframe):\r\n",
        "  PT = PowerTransformer(standardize = False,method = 'yeo-johnson')\r\n",
        "  pt_data = PT.fit_transform(dataframe)\r\n",
        "  pt_df = pd.DataFrame(data = pt_data, columns = dataframe.columns)\r\n",
        "  return pt_df"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcUJ0Z23vV0n"
      },
      "source": [
        "### 평균으로 Missing Value 보간"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKVZIu4wvb0h"
      },
      "source": [
        "def imputeNull(dataframe):\r\n",
        "  for i in dataframe.columns:\r\n",
        "    dataframe[i].fillna(dataframe[i].mean(), inplace = True)\r\n",
        "    # ddataframef[i].fillna(0, inplace = True)\r\n",
        "  return dataframe"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4_hsNEhvdz9"
      },
      "source": [
        "### PCA를 통한 차원 축소"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJD8pS6LvddN"
      },
      "source": [
        "def decomposition(feature_df, method = 'pca', decom_rs = 0):\r\n",
        "  # feature_df, label_df = featureLabelSplit(dataframe)\r\n",
        "  if method == 'pca':\r\n",
        "    pca = PCA(n_components = None)\r\n",
        "    principalComponents = pca.fit_transform(feature_df) #### DataFrame\r\n",
        "    df = pd.DataFrame(principalComponents)\r\n",
        "  elif method == 'ica':\r\n",
        "    ica = FastICA(n_components=50,random_state=decom_rs,max_iter = 1000000)\r\n",
        "    principalComponents = ica.fit_transform(feature_df)\r\n",
        "    df = pd.DataFrame(principalComponents)\r\n",
        "  return df"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AGVWqIkvoFm"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrAzXBIWvuu4"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP41IVA4t2Zq"
      },
      "source": [
        "## Load Data\r\n",
        "raw_df = pd.read_csv(\"./uci-secom.csv\")\r\n",
        "raw_df.drop(['Time'], axis = 1, inplace = True)\r\n",
        "\r\n",
        "## Split Target\r\n",
        "feats, label = featureLabelSplit(raw_df)\r\n",
        "\r\n",
        "## 상관관계 0.03이상인 feature 추출\r\n",
        "high_corr_features = get_high_corr_indicies(raw_df, 0.03)\r\n",
        "\r\n",
        "## 값이 하나인 피쳐 제거\r\n",
        "unique_feats = removeUnique(feats, high_corr_features)\r\n",
        "\r\n",
        "## 결측치 많은 피쳐 제거\r\n",
        "less_null_feats = dropNullDominant(unique_feats, high_corr_features, ratio=0.0)\r\n",
        "\r\n",
        "## 피쳐 스케일링\r\n",
        "scaled_feats = robustScaler(less_null_feats)\r\n",
        "\r\n",
        "## 피쳐 정규분포화\r\n",
        "power_transformed_feats = powerTransform(scaled_feats)\r\n",
        "\r\n",
        "## 전체 평균으로 결측치 대체\r\n",
        "mean_df = imputeNull(power_transformed_feats)\r\n",
        "\r\n",
        "## 차원 축소\r\n",
        "dec_df = decomposition(mean_df,'pca')\r\n",
        "\r\n",
        "## feature label 합체\r\n",
        "target_df = featureLabelMerge(dec_df,label,'Pass/Fail')\r\n",
        "\r\n",
        "## 높은 상관관계(0.039) 가진 애들만 추출\r\n",
        "high_corr_features = get_high_corr_indicies(target_df, 0.039)\r\n",
        "high_corr_data = target_df[high_corr_features.index]\r\n",
        "\r\n",
        "## neg / pos 비율 계산\r\n",
        "raw_dff = raw_df.copy()\r\n",
        "raw_dff['Pass/Fail'] = raw_df['Pass/Fail'].replace({-1:0})\r\n",
        "pos, neg = np.bincount(raw_dff['Pass/Fail'])\r\n",
        "total = neg + pos"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdP4cXp_v_is"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyVfm7aRuB_Z"
      },
      "source": [
        "## Early Stopping\r\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\r\n",
        "    monitor='val_auc', \r\n",
        "    verbose=0,\r\n",
        "    patience=10,\r\n",
        "    mode='max',\r\n",
        "    restore_best_weights=True)\r\n",
        "\r\n",
        "## Metrics\r\n",
        "METRICS = [\r\n",
        "      keras.metrics.TruePositives(name='tp'),\r\n",
        "      keras.metrics.FalsePositives(name='fp'),\r\n",
        "      keras.metrics.TrueNegatives(name='tn'),\r\n",
        "      keras.metrics.FalseNegatives(name='fn'), \r\n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\r\n",
        "      keras.metrics.Precision(name='precision'),\r\n",
        "      keras.metrics.Recall(name='recall'),\r\n",
        "      keras.metrics.AUC(name='auc'),\r\n",
        "  ]\r\n",
        "\r\n",
        "## Model 선언\r\n",
        "def make_model(metrics=METRICS, output_bias=None):\r\n",
        "  if output_bias is not None:\r\n",
        "    output_bias = tf.keras.initializers.Constant(output_bias)\r\n",
        "  model = keras.Sequential([\r\n",
        "      keras.layers.Dense(512 , input_shape=(46,), activation= tf.keras.layers.LeakyReLU(alpha=0.5),),\r\n",
        "      keras.layers.BatchNormalization(),\r\n",
        "      keras.layers.Dense(1, activation='sigmoid',\r\n",
        "                         bias_initializer=output_bias),\r\n",
        "  ])\r\n",
        "  model.compile(\r\n",
        "      optimizer=keras.optimizers.Adam(lr=1e-3),\r\n",
        "      loss=keras.losses.BinaryCrossentropy(),\r\n",
        "      metrics=metrics)\r\n",
        "\r\n",
        "  return model\r\n",
        "\r\n",
        "## Learning parameter\r\n",
        "EPOCHS = 300\r\n",
        "BATCH_SIZE = 2048\r\n",
        "\r\n",
        "## initial bias\r\n",
        "initial_bias = np.log([pos/neg])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2tEjj0gwRmc"
      },
      "source": [
        "### Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "QoTZp0QnwRFp",
        "outputId": "92a2c8c8-9b18-4165-d377-6a49cd88bc8e"
      },
      "source": [
        "## 학습 진행\r\n",
        "sum = 0\r\n",
        "for i in range(1,101): \r\n",
        "  train_df, test_df = train_test_split(high_corr_data, test_size=0.2, random_state = i , shuffle =True, stratify = high_corr_data['Pass/Fail'])\r\n",
        "  train_df, val_df = train_test_split(train_df, test_size=0.2, random_state = 1 , shuffle = True, stratify = train_df['Pass/Fail'])\r\n",
        "  \r\n",
        "  train_features = train_df.drop(\"Pass/Fail\",1) \r\n",
        "  val_features = val_df.drop(\"Pass/Fail\",1)\r\n",
        "  test_features = test_df.drop(\"Pass/Fail\",1)\r\n",
        "  train_labels = train_df['Pass/Fail'] \r\n",
        "  val_labels = val_df['Pass/Fail'] \r\n",
        "  test_labels = test_df['Pass/Fail']\r\n",
        "  train_labels= train_labels.replace({-1:0})\r\n",
        "  val_labels= val_labels.replace({-1:0})\r\n",
        "  test_labels= test_labels.replace({-1:0})\r\n",
        "  \r\n",
        "  weight_for_0 = (1 / neg)*(total)/2.0 \r\n",
        "  weight_for_1 = (1 / pos)*(total)/2.0\r\n",
        "\r\n",
        "  class_weights = {0: weight_for_0, 1: weight_for_1}\r\n",
        "  \r\n",
        "  test_model = make_model(output_bias=initial_bias)\r\n",
        "  #test_model.load_weights(initial_weights)\r\n",
        "  weighted_history = test_model.fit(\r\n",
        "      train_features,\r\n",
        "      train_labels,\r\n",
        "      batch_size=BATCH_SIZE,\r\n",
        "      epochs=EPOCHS,\r\n",
        "      callbacks=[early_stopping],\r\n",
        "      validation_data=(val_features, val_labels),\r\n",
        "      class_weight=class_weights,\r\n",
        "      verbose = 0)\r\n",
        "  train_predictions = test_model.predict(train_features, batch_size=BATCH_SIZE)\r\n",
        "  test_predictions = test_model.predict(test_features, batch_size=BATCH_SIZE)\r\n",
        "  results = test_model.evaluate(test_features, test_labels,batch_size=BATCH_SIZE, verbose=0)\r\n",
        "  sum += results[-1]\r\n",
        "  print(\"{:2}\\tauc = {:.4f}\\t----avg auc ={:.4f}\".format(i, results[-1],sum/i))\r\n",
        "print(sum/100)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1\tauc = 0.8401\t----avg auc =0.8401\n",
            " 2\tauc = 0.9185\t----avg auc =0.8793\n",
            " 3\tauc = 0.8276\t----avg auc =0.8621\n",
            " 4\tauc = 0.8176\t----avg auc =0.8509\n",
            " 5\tauc = 0.8181\t----avg auc =0.8444\n",
            " 6\tauc = 0.7193\t----avg auc =0.8235\n",
            " 7\tauc = 0.8292\t----avg auc =0.8243\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-56dd9af7fe9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m       verbose = 0)\n\u001b[0m\u001b[1;32m     33\u001b[0m   \u001b[0mtrain_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mtest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1139\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m   1142\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1387\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       return self._concrete_stateful_fn._call_flat(\n\u001b[0;32m--> 895\u001b[0;31m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_kwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_filtered_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnWg2UIGwUe_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}